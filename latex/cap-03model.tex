%% ------------------------------------------------------------------------- %%
\chapter{The endpoint-inflated beta-binomial regression model}
\label{cap:model}

In this chapter we introduce our formulation of the EIBB regression model. We will first present the full model likelihood and a set of suggested priors for Bayesian inference. We finish with a brief discussion of applicable model comparison criteria.

%% ------------------------------------------------------------------------- %%
\section{Model definition}
\label{sec:defmodel}

Let $Y_i = \begin{pmatrix}Y_{i1} & ... & Y_{iN_i}\end{pmatrix}^\top$, $i = 1,...,N$ be a set of independent response vectors where

\begin{equation}
Y_{ij} \sim \mathrm{EIBB}(\mu_{ij}, \phi_{ij}, p^{_0}_{ij}, p^{_1}_{ij}, p^{_2}_{ij}; n_{ij}).
\end{equation}
	
The dependence of model parameters on covariates is given by the following general structure:

\begin{equation}
\begin{split}
\mu_{ij} &= h_1(x_{ij}^\top \beta + z_{ij}^\top b_i)\\
\phi_{ij} &= h_2(\hat{x}_{ij}^{\top} \gamma + \hat{z}_{ij}^{\top} g_i)\\
\begin{pmatrix}p_{ij}^{_0} & p_{ij}^{_1} & p_{ij}^{_2}\end{pmatrix}^\top &= h_3(\dot{x}_{ij}^{\top} \dot{\delta}+\dot{z}_{ij}^{\top} \dot{d}_i, \ddot{x}_{ij}^{\top} \ddot{\delta}+\ddot{z}_{ij}^{\top} \ddot{d}_i)
\end{split}
\label{regression-model}
\end{equation}

\noindent where $\beta, \gamma, \dot{\delta}, \ddot{\delta}$ are vectors of fixed effects and $b_i, g_i, \dot{d}_i, \ddot{d}_i$ are vectors of random effects. The vectors $x_{ij}, \hat x_{ij}, \dot x_{ij}, \ddot x_{ij}$ hold covariate values.

In order to remain within parameter space, the link functions must be defined so that

\begin{equation}
\begin{split}
h_1:&\: \mathbb R\; \to (0,1)\\
h_2:&\: \mathbb R\; \to \mathbb R^+\\
h_3:&\: \mathbb R^2 \to (0,1)^3\\
&\: \forall j,k \in \mathbb R \left( \vec 1 \cdot h_3(j,k) = 1 \right).
\end{split}
\label{link-definitions}
\end{equation}

We will take the usual selecton of logit for $h_1$ and log for $h_2$. To motivate our choice for the link function on the mixture proportions, $h_3$, note that the mixture components can be ordered in terms of their expected values: $0<n\mu<n$, for the first, second and third components respectively. We will relax this interpretation soon, but for now, see that the latent $Z$ in \ref{eibb-distribution} which selects the mixture components can be regarded as an ordered categorical variable. Then the ordered probit model gives us a starting point by which to introduce a linear predictor on its parameters $p_{ij}^{_0}$, $p_{ij}^{_1}$, $p_{ij}^{_2}$.

If $\Phi(.;\mu',\sigma'^2)$ is the cumulative distribution function of a normal with mean $\mu'$ and variance $\sigma'^2$ and $c_0$, $c_1$ are two real numbers such that $c_0 < c_1$, the ordered probit regression model is given by

\begin{equation}
\begin{split}
p^{_0} &= \Phi(c_0;\mu'_{ij},\sigma'^{2}) \\
p^{_1} &= \Phi(c_1;\mu'_{ij},\sigma'^{2}) - \Phi(c_0;\mu'_{ij},\sigma'^{2})\\
p^{_2} &= 1 - \Phi(c_1;\mu'_{ij},\sigma'^{2})\\
\mu'_{ij} &= \dot{x}_{ij}^{\top} \dot{\delta}+\dot{z}_{ij}^{\top} \dot{d}_i
\end{split}
\label{eq:oprobit}
\end{equation}

where $x'^\top \nu$ holds the fixed effects terms and $z'^\top v$ are any random effects terms. Given the restriction $\sum_s p_{ij}^{_s} = 1$, we see that the above system relates two free parameters with four unknowns. Therefore some values must be fixed to obtain a unique solution. We choose to set $c_0 = 0$ and $c_1 = 1$. Figure \ref{fig:oprobit} illustrates this setup.

The assumption of a fixed $\sigma'^{2}$ can now be relaxed by setting

\begin{equation}
\begin{split}
\sigma'^{2}_{ij} &= log(\ddot{x}_{ij}^{\top} \ddot{\delta}+\ddot{z}_{ij}^{\top} \ddot{d}_i).
\end{split}
\label{eq:oprobit2}
\end{equation}

This retains the flexibility of the more common softmax link function while offering a latent variable interpretation for the inflation behavior.

\begin{figure}
  \includegraphics[width=\linewidth]{oprobit.png}
  \caption{The black curve shows a $\mu'$, $\sigma'^{2}$ normal cdf and how the cutpoints $c_0$, $c_1$ are used to retrieve the vector of probabilities. The corresponding normal pdf is shown in gray.}
  \label{fig:oprobit}
\end{figure}

%% ------------------------------------------------------------------------- %%
\section{Bayesian Inference}

As recommended in \cite{Gelman2013}, we use weakly informative priors for all coefficients in the model. For all data where it would seem reasonable to apply this model, the linear predictor for unit interval parameters, under the link functions discussed above, should contain coefficients with magnitudes in the single digits (provided covariates are first standardized). Therefore, zero-centered normal priors with $\sigma=10$ are a reasonable option.

Priors associated to dispersion/precision parameters should keep mass away from zero and allow large values. Cauchy priors fulfill this requirement for coefficients in the linear predictors, but an improper prior for the intercepts is best, as there is no reason to assume they are located around zero.

Though sometimes a full set of improper priors is used as a way of "letting the data speak for itself",  note that such a choice does not guarantee a proper posterior will result. \cite{Tak2015} Briefly, one should verify that interiors group exist, i.e. all the data should not be located at the endpoints, and that there are at least as many observations as there are parameters. If proper priors are employed, these issues are avoided altogether.

%% ------------------------------------------------------------------------- %%
\section{Measures for model assessment}
\label{sec:modcomp}

\subsection{Information criteria}

Information criteria provide a way to compare the relative predictive prowess of two or more models. Suitable choices under a Bayesian paradigm include Deviance Information Criterion (DIC), Widely Applicable Information Criterion (WAIC), Expected Akaike Information Criterion (EAIC) and Expected Bayesian Information Criteria (EBIC). The key quantity used to calculate them is the deviance, defined as

\begin{equation}
\mathcal{D}(L)=-2\log\{L\}
\end{equation}

\noindent where $L$ is the model's likelihood.

We now provide the formulas for the information criteria, using a bar to denote posterior means.

\begin{itemize}
\item $\text{DIC}=\mathcal{D}(\bar L) + 2(\bar{\mathcal{D}} (L) - \mathcal{D} (\bar L))$
\item $\text{WAIC}=\mathcal{D}(\bar L) + \overline{\text{Var}}(\log L)$
\item $\text{EAIC}=\mathcal{D}(\bar L) + p$
\item $\text{EBIC}=\mathcal{D}(\bar L) + p \times \log (n)$
\end{itemize}

In the last two equations, note that $p$ and $n$ stand for number of parameters and observations, respectively.

Because deviance is minimized when parameters allow a perfect fit to the data, models that score lower are to be preferred. The second term in the sum of each criterion are corrections which penalize models for complexity, such that from two models with equal deviance, the one which uses less parameters is favored.

Some issues are the size of what may be considered meaningful differences and that, as these are relative measures of performance, selecting the model with lowest information criteria from a set of candidates provides no guarantee that the model is in fact a good fit for the data at hand. The next section provides a test that may be used to evaluate overall fit, independently of alternative reference models.

\subsection{Bayesian $\chi^2$}

\cite{johnson2004bayesian} proposes a test for a goodness-of-fit testing in a Bayesian setting based on the classical $\chi^2$ test. The test statistic for a discrete random variable model is given by

\begin{equation}
R^B(\widetilde{\theta}) = \sum_{k=1}^K \left[{m_k - Np_k(\widetilde{\theta}) \over \sqrt{Np_k(\widetilde{\theta})}}\right]^2
\end{equation}

where $p_k(\widetilde{\theta}) = \frac{1}{N} \sum_{j=1}^N\sum_{y\in \text{bin}k}f_j(y \mid \widetilde{\theta})$ denotes the sum of pmfs $f_j$ evaluated at each observation, conditioned on $\widetilde{\theta}$, a single draw from the posterior parameter vector. The $K$ bins over which the sum takes place correspond to the possible values the random variable may take and $m_k$ is the actual number of such values observed.

The distribution of $R^B(\widetilde{\theta})$ converges to a $\chi^2$ with $K-1$ degrees of freedom as $n \rightarrow \infty$, provided regularity conditions are met and parameter draws are independent. Although this last requirement is violated when repeated draws from the posterior are made based on the sample data sample, the approximation is often good enough. 

To measure fit, one simply draws samples of $R^B$ and a  $\chi^2_{K-1}$ random variable, then compares the proportion of test statistic values which exceed the reference distribution. Large deviations from a value of $0.5$ are then indicative of bad model fit, without need for providing another reference model.

The use of both types of measures will be illustrated in the simulation and application sections which follow.